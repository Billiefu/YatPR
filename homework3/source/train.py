"""
@module train
@function åŠç›‘ç£å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸»ç¨‹åºï¼Œæ”¯æŒFixMatchå’ŒMixMatchç®—æ³•
@author å‚…ç¥‰ç
@date 2025å¹´5æœˆ23æ—¥
"""

import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # è§£å†³libiomp5md.dllå†²çªé—®é¢˜

import gc
import random

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import trange

from dataloader import get_ssl_dataloaders
from fixmatch import fixmatch_step
from mixmatch import mixmatch_step
from models.wideresnet import WideResNet
from utils import set_seed, plot_curves, plot_loss

# =================== é…ç½®å‚æ•° =====================
method = "fixmatch"  # è®­ç»ƒæ–¹æ³•é€‰æ‹©ï¼š"mixmatch"æˆ–"fixmatch"
batch_size = 64  # æ‰¹æ¬¡å¤§å°
num_epochs = 1024  # è®­ç»ƒæ€»è½®æ•°
learning_rate = 0.002  # å­¦ä¹ ç‡
lambda_u = 75 if method == "mixmatch" else 1  # æ— ç›‘ç£æŸå¤±æƒé‡
accumulation_steps = 1  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
val_check_interval = 100  # éªŒè¯é—´éš”
patience = 10  # æ—©åœè€å¿ƒå€¼
save_path = f"./models/{method}.pth"  # æ¨¡å‹ä¿å­˜è·¯å¾„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è®¾å¤‡é€‰æ‹©

# =================== æ•°æ®è·¯å¾„ =====================
train_csv = "datasets/cifar10_train_4000.csv"  # è®­ç»ƒé›†CSVè·¯å¾„
val_csv = "datasets/cifar10_val_4000.csv"  # éªŒè¯é›†CSVè·¯å¾„
test_csv = "datasets/cifar10_test.csv"  # æµ‹è¯•é›†CSVè·¯å¾„

# =================== è¶…å‚æ•° =====================
num_labeled = 250  # ä½¿ç”¨çš„æœ‰æ ‡ç­¾æ ·æœ¬æ•°é‡

# =================== åˆå§‹åŒ–è®¾ç½® =====================
set_seed(42)  # è®¾ç½®éšæœºç§å­
torch.backends.cudnn.benchmark = True  # å¯ç”¨CuDNNåŸºå‡†æµ‹è¯•


def split_labeled_unlabeled(train_csv, num_labeled, seed=42):
    """ åˆ’åˆ†æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®é›†

    å‚æ•°ï¼š
    train_csv (str): åŸå§‹è®­ç»ƒé›†CSVè·¯å¾„
    num_labeled (int): éœ€è¦çš„æœ‰æ ‡ç­¾æ ·æœ¬æ•°é‡
    seed (int): éšæœºç§å­ï¼Œé»˜è®¤ä¸º42

    è¿”å›ï¼š
    tuple: (æœ‰æ ‡ç­¾æ•°æ®é›†è·¯å¾„, æ— æ ‡ç­¾æ•°æ®é›†è·¯å¾„)
    """
    df = pd.read_csv(train_csv)
    random.seed(seed)

    labeled_indices = []
    n_per_class = num_labeled // 10  # æ¯ç±»æ ·æœ¬æ•°

    # æŒ‰ç±»åˆ«å‡è¡¡é‡‡æ ·
    for c in range(10):
        cls_indices = df[df['label'] == c].index.tolist()
        sampled = random.sample(cls_indices, n_per_class)
        labeled_indices.extend(sampled)

    unlabeled_indices = list(set(df.index) - set(labeled_indices))

    # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
    labeled_df = df.loc[labeled_indices].reset_index(drop=True)
    unlabeled_df = df.loc[unlabeled_indices].reset_index(drop=True)

    labeled_path = "datasets/temp_labeled.csv"
    unlabeled_path = "datasets/temp_unlabeled.csv"
    labeled_df.to_csv(labeled_path, index=False)
    unlabeled_df.to_csv(unlabeled_path, index=False)

    return labeled_path, unlabeled_path


def main():
    """ ä¸»è®­ç»ƒå‡½æ•° """
    # 1. æ•°æ®å‡†å¤‡
    labeled_csv, unlabeled_csv = split_labeled_unlabeled(train_csv, num_labeled=num_labeled)

    # è·å–æ•°æ®åŠ è½½å™¨
    labeled_loader, unlabeled_loader, val_loader, test_loader = get_ssl_dataloaders(
        labeled_csv, unlabeled_csv, val_csv, test_csv, batch_size=batch_size
    )

    # 2. æ¨¡å‹åˆå§‹åŒ–
    model = WideResNet(depth=28, widen_factor=2, num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # 3. è®­ç»ƒçŠ¶æ€åˆå§‹åŒ–
    best_val_acc, best_model = 0.0, None
    no_improve_epochs = 0
    train_loss_list, test_loss_list, train_acc_list, test_acc_list = [], [], [], []

    # 4. è®­ç»ƒå¾ªç¯
    for epoch in trange(num_epochs):
        model.train()
        total_loss, total_x, total_u = 0.0, 0.0, 0.0
        correct, total = 0, 0

        # æ‰¹æ¬¡è®­ç»ƒ
        for batch_idx, (labeled_batch, unlabeled_batch) in enumerate(zip(labeled_loader, unlabeled_loader)):
            try:
                with torch.amp.autocast(enabled=True, device_type='cuda'):
                    # æ ¹æ®æ–¹æ³•é€‰æ‹©è®­ç»ƒæ­¥éª¤
                    if method == "mixmatch":
                        loss, loss_x, loss_u, preds, targets = mixmatch_step(
                            model, optimizer, labeled_batch,
                            unlabeled_batch, criterion, device, lambda_u=lambda_u
                        )
                    elif method == "fixmatch":
                        loss, loss_x, loss_u, mask_rate, preds, targets = fixmatch_step(
                            model, optimizer, labeled_batch,
                            unlabeled_batch[0], unlabeled_batch[1],
                            criterion, device, lambda_u=lambda_u
                        )
                    else:
                        raise ValueError(f"Unsupported method: {method}")

                # ç»Ÿè®¡æŸå¤±
                total_loss += loss.item() if isinstance(loss, torch.Tensor) else loss
                total_x += loss_x.item() if isinstance(loss_x, torch.Tensor) else loss_x
                total_u += loss_u.item() if isinstance(loss_u, torch.Tensor) else loss_u

                # è®¡ç®—å‡†ç¡®ç‡
                _, y_l = labeled_batch
                y_l = y_l.to(device)
                pred_labels = preds.argmax(dim=1)
                correct += (pred_labels[:len(y_l)] == y_l).sum().item()
                total += len(y_l)

            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    print(f"âŒ CUDA OOM at batch {batch_idx}, skipping...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    continue
                else:
                    raise e

        # 5. è®°å½•è®­ç»ƒæŒ‡æ ‡
        avg_loss = total_loss / min(len(labeled_loader), len(unlabeled_loader))
        train_loss_list.append(avg_loss)
        train_acc = 100. * correct / total
        train_acc_list.append(train_acc)

        # 6. éªŒè¯é˜¶æ®µ
        val_acc, val_loss = evaluate(model, val_loader, criterion, device)
        print(f"âœ… Epoch {epoch + 1}: Train Loss {avg_loss:.4f} | Train Acc {train_acc:.2f}% | Val Acc {val_acc:.2f}%")

        # 7. æ—©åœæœºåˆ¶
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model = model.state_dict()
            no_improve_epochs = 0
        else:
            no_improve_epochs += 1
            if no_improve_epochs >= patience:
                print("â¹ï¸ Early stopping triggered.")
                break

        # 8. å®šæœŸæµ‹è¯•
        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:
            test_acc, test_loss = evaluate(model, test_loader, criterion, device)
            test_acc_list.append(test_acc)
            test_loss_list.append(test_loss)
            print(f"ğŸ“Š Test Accuracy: {test_acc:.2f}%, Test Loss: {test_loss:.4f}")

        # æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()

    # 9. ä¿å­˜ä¸å¯è§†åŒ–
    if best_model:
        model.load_state_dict(best_model)
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Best model saved to {save_path}")

    plot_curves(train_acc_list, test_acc_list, save_path=f"result/{method}_acc_curve.png")
    plot_loss(train_loss_list, test_loss_list, save_path=f"result/{method}_loss_curve.png")


def evaluate(model, dataloader, criterion, device):
    """ æ¨¡å‹è¯„ä¼°å‡½æ•°

    å‚æ•°ï¼š
    model (nn.Module): å¾…è¯„ä¼°æ¨¡å‹
    dataloader (DataLoader): æ•°æ®åŠ è½½å™¨
    criterion (nn.Module): æŸå¤±å‡½æ•°
    device (torch.device): è®¡ç®—è®¾å¤‡

    è¿”å›ï¼š
    tuple: (å‡†ç¡®ç‡, å¹³å‡æŸå¤±)
    """
    model.eval()
    correct, total = 0, 0
    total_loss = 0.0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = torch.argmax(outputs, dim=1)

            loss = criterion(outputs, labels)
            total_loss += loss.item()

            total += labels.size(0)
            correct += (preds == labels).sum().item()

    acc = 100. * correct / total
    avg_loss = total_loss / len(dataloader)
    return acc, avg_loss


if __name__ == "__main__":
    main()
